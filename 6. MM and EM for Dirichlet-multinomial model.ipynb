{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM and EM for Dirichlet-multinomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. Here we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "From the question1 in hw4, we know\n",
    "$$\n",
    "    f(\\mathbf{x} \\mid \\alpha) \n",
    "\t= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p}  \n",
    "    = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$\n",
    "$$\\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} = \\prod_{j=1}^d a_j(a_j+1)...(a_j+x_j-1)$$\n",
    "$$\\prod_{j=1}^d \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)} = \\prod_{j=1}^d |a|(|a|+1)...(|a|+|x|-1)$$\n",
    "\n",
    "$$f(\\mathbf{x} \\mid \\alpha) = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d a_j(a_j+1)...(a_j+x_j-1) \\prod_{j=1}^d |a|(|a|+1)...(|a|+|x|-1)$$\n",
    "\n",
    "$$L(\\alpha) = ln f(\\mathbf{x_1,x_2,...x_n} \\mid \\alpha) = \\sum_{i=1}^n ln \\binom{|\\mathbf{x}|}{\\mathbf{x}} + \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_ij-1}ln(a_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|x_i|-1} ln(|a|+k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "Differentiate the identity:\n",
    "$$\\int \\pi(\\mathbf{p}) d \\mathbf{p} = \\int \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} d \\mathbf{p} = 1 $$\n",
    "\n",
    "$$\\int \\prod_{j=1}^d p_j^{\\alpha_j-1} d \\mathbf{p} = \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)} \\dots equation 1$$\n",
    "\n",
    "$$E(lnP_j)=\\int \\frac{\\Gamma(|a|)}{\\Gamma(a)j)} P_{j}^{a_j-1} ln (P_j) dp$$\n",
    "\n",
    "Let's set $\\frac{\\Gamma(|a|)}{\\Gamma(a)j)}$ as $h(a)$\n",
    "\n",
    "According to the Leibniz integral rule:\n",
    "$$\\frac{\\partial{}}{\\partial{x}} \\int_{a}^{b}f(x,t)dt = \\int_{a}^{b} \\frac{\\partial{}}{\\partial{x}} f(x,t)dt $$\n",
    "Therefore, if we let x in the above formular to be the $a_j$, and $f(x,t) as the P_j^{a_j-1}$. Then we can get\n",
    "$$\\frac{\\partial{}}{\\partial{a_j}} \\int \\prod_{j=1}^d P_j^{a_j-1} dp = \\int P_j^{a_j-1} log(P_j) dP_j = \\int h(a) P_j^{a_j-1}  ln(P_j) \\frac{1}{h(a)} dp$$\n",
    "$$= E(lnP_j).\\frac{1}{h(a)} \\dots equation2$$\n",
    "\n",
    "Then let's take derivative of $a_j$ the left part of equation1:\n",
    "$$\\frac{\\partial{}}{\\partial{a_j}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)}$$\n",
    "$$= \\frac{\\partial{}}{\\partial{a_j}} e^{log(\\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)})}$$\n",
    "$$= \\frac{\\partial{}}{\\partial{a_j}} e^{\\sum_{j=1}^d (ln\\Gamma(\\alpha_j)-ln\\Gamma(|\\alpha|))} \\dots equation3$$\n",
    "\n",
    "If we set $f(a_j)= {\\sum_{j=1}^d (ln\\Gamma(\\alpha_j)-ln\\Gamma(|\\alpha|))}$\n",
    "Then equation3 equals:\n",
    "\n",
    "$$\\frac{\\partial{}}{\\partial{a_j}} e^{f(a_j)} = e^{f(a_j)} \\frac{\\partial{}}{\\partial{a_j}} f(a_j)$$\n",
    "\n",
    "$$= \\frac{1}{h(a)}.[\\frac{P'(a_j)}{P(a_j)}-\\frac{P'|a|)}{P(|a|)}]$$\n",
    "$$=\\frac{1}{h(a)}.[\\Psi(a_j)-\\Psi(|a|)] \\dots equation4$$\n",
    "\n",
    "Because equation2 equals to equation4,  so we prooved $\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "The Q function is \n",
    "$$Q(\\alpha|\\alpha^{(t)})=E[ln\\prod_{i=1}^nf(x_i,p_i,\\alpha)|x_1,\\dots,x_n,a^{(t)}]$$\n",
    "\n",
    "The expectation is with respect to the conditional distribution\n",
    "$$f(p_1,\\dots,p_n|x_1,\\dots,x_n,a^{(t)})$$\n",
    "\n",
    "independent Dirichlet$(x_i+\\alpha^{(t)})$\n",
    "$$Q(\\alpha|\\alpha^{(t)}) = \\sum_{i=1}^n Q_i(\\alpha|\\alpha^{(t)})$$\n",
    "\n",
    "with $$Q_i(\\alpha|\\alpha^{(t)}) = \\int_{\\Delta_d} \\frac{\\Gamma(m_i+|\\alpha|^{(t)})}{\\prod_{j=1}^d \\Gamma(x_{ij}+\\alpha_j^{(t)})} \\prod_j p_{ij}^{x_{ij}+a_j^{(t)}-1}$$\n",
    "$$.[ln \\binom{\\mathbf{m}_i}{\\mathbf{x}_i} + \\sum_{j=1}^d(x_{ij}+a)j-1)lnp_{ij}+ln\\Gamma(|\\alpha|)- \\sum_{j=1}^dln\\Gamma(a_j)]dp_i$$\n",
    "\n",
    "$$=ln \\binom{\\mathbf{m}_i}{\\mathbf{x}_i} + \\sum_{j=1}^d(x_{ij}+a)j-1) [\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(m_i+|\\alpha^{(t)}|)]  + ln\\Gamma(|\\alpha|)- \\sum_{j=1}^dln\\Gamma(a_j)$$\n",
    "\n",
    "Therefore we can get \n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "We first need to simplify the two sums:\n",
    "$$\\sum_{i=1}^{n} \\sum_{j=1}^{d} \\sum_{k=0}^{x_{ij}-1} ln(\\alpha_j +k) = \\sum_{i=1}^{n} \\sum_{j=1}^{d} \\sum_{k=0}^{max_i x_{ij}-1}   ln(\\alpha_j+k) 1_{\\{x_{ij}\\geq k\\}}$$\n",
    "$$= \\sum_{j=1}^{d} \\sum_{k=0}^{max_i x_{ij}-1} ln(\\alpha_j+k) \\sum_{i=1}^{n} 1_{\\{x_{ij}\\geq k\\}}$$\n",
    "$$= \\sum_{j=1}^{d} \\sum_{k=0}^{max_i x_{ij}-1} s_{jk} ln(\\alpha_j+k)$$\n",
    "\n",
    "and\n",
    "$$-\\sum_{i=0}^{n}\\sum_{k=0}^{m_i-1} ln(|\\alpha|+k) = -\\sum_{k=0}^{m_i-1}ln(|\\alpha|+k) \\sum_{i=1}^n 1_{\\{k\\leq m_i-1\\}}$$\n",
    "$$= - \\sum_{k=0}^{max_i m_i-1} r_k ln(|\\alpha|+k) $$\n",
    "\n",
    "Applying the Jensen's inequality to the $ln(a_j+k)$ terms\n",
    "\n",
    "$$ln(a_j+k) \\geq \\frac{a_j^{(t)}}{a_j^{(t)}+k} ln(\\frac{a_j^{(t)}+k}{a_j^{(t)}} . a_j) + \\frac{k}{a_j^{(t)}+k} ln(\\frac{a_j^{(t)}+k}{k} . k) = \\frac{a_j^{(t)}}{a_j^{(t)}+k} lna_j + c^{(t)} $$\n",
    "\n",
    "and the supporting hyperplane inequality to the $-ln(|a|+k)$ terms\n",
    "\n",
    "$$-ln(|a|+k) \\geq -\\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k} -ln(|a^{(t)}|+k) = -\\frac{|\\alpha|}{|\\alpha^{(t)}|+k} +c^{(t)}$$\n",
    "\n",
    "Combining all the above we get the minorizing function\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X::AbstractMatrix; \n",
    "    α0::Vector = dirmult_mom(X), \n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6\n",
    "    )\n",
    "    \n",
    "    # \n",
    "    # set default starting point from Q7\n",
    "    α0 = dirmult_mom(X)\n",
    "    \n",
    "    T = promote_type(eltype(X), eltype(α0))\n",
    "    #remove rows and columns with sum <0\n",
    "    rowind = find(sum(X,1))\n",
    "    colind = find(sum(X,2))\n",
    "    Xwork = @view X[rowind, colind]\n",
    "    αwork = α[rowind]\n",
    "    \n",
    "    # pre-allocate variables\n",
    "    α = similar(αwork)\n",
    "    ∇ = similar(αwork)\n",
    "    newtondir = similar(αwork)\n",
    "    αnew = similar(αwork)\n",
    "    obsinfo_dinv =zero(T)\n",
    "    alphaHat = alpha0\n",
    "    alphaSum = sum(alphaHat)\n",
    "    logl = sum(weights * ddirmult(data, alpha0, log = TRUE))\n",
    "\n",
    "    \n",
    "    #Sj = function(xj, kvec, weight) \n",
    "    Sj = sum(weight * outer(xj, kvec, \">\"),1)\n",
    "    scts = each_col!(data, Sj, kvec = 0:(max(data) - 1), weight = weights)\n",
    "    rcts = Sj(m, kvec = 0:(max(m) - 1), weight = weights)\n",
    "\n",
    "    ## MM loop\n",
    "        for iter in 2:maxiters\n",
    "          \n",
    "          # MM update\n",
    "          num = sum(scts / (outer(0:(max(data) - 1), alphaHat, \"+\")),1)\n",
    "          den = sum(rcts / (sum(alphaHat) + (0:(max(m) - 1))))\n",
    "          alphaHat = alphaHat. * num / den\n",
    "          \n",
    "          # log-likelihood at new iterate\n",
    "          loglOld = loglIter\n",
    "          loglIter = sum(weights * ddirmult(data, alphaHat, log = TRUE))\n",
    "          \n",
    "          # Print the iterate log-like if requested\n",
    "          if verbose\n",
    "            println(\"iteration \", iter, \", logL = \", loglIter)\n",
    "          \n",
    "          # check convergence criterion\n",
    "          if abs(loglIter - loglOld) < tolfun * (abs(loglOld) + 1)\n",
    "            break  \n",
    "                \n",
    "          # score, i.e., gradient\n",
    "          alphaMat = matrix(alphaHat, nrow = N, ncol = d, byrow = TRUE)\n",
    "          score = colSums(weights * digamma(data + alphaMat)) - \n",
    "          weightsum * (digamma(alphaHat) - digamma(alphaSum)) -\n",
    "          sum(weights * digamma(alphaSum + m))\n",
    "\n",
    "                \n",
    "          return(estimate = alphaHat, se = se, maximum = loglIter, iterations = iter, \n",
    "          gradient = score, obsinfo = obsinfo, obsinfoInv = obsinfoInv)\n",
    "  }\n",
    "\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training data\n",
    "tra1 = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "tra2 = \"-2018spring/hw/hw4/optdigits.tra\"\n",
    "\n",
    "train = readdlm(download(tra1 * tra2), ',')\n",
    "\n",
    "# pre-allocate variables\n",
    "loglNewton = zeros(10)\n",
    "itersNewton = zeros(10)\n",
    "runtimeNewton = zeros(10)\n",
    "loglMM = zeros(10)\n",
    "itersMM = zeros(10)\n",
    "runtimeMM = zeros(10)\n",
    "digitCount = zeros(10)\n",
    "\n",
    "# loop over digit\n",
    "for dig in 0:9\n",
    "  # retrieve digit data\n",
    "  digitdata = traindata[traindata[:, 65]. == dig, 1:64]'\n",
    "  digitCount[dig + 1] = size(Xd, 2)\n",
    "  # Newton method\n",
    "  digNewton <- dirmultfit(digitdata, algorithm = \"Newton\")\n",
    "  benchmark(dirmultfit(digitdata, algorithm = \"Newton\"))\n",
    "    \n",
    "  # MM algorithm\n",
    "  digMM <- dirmultfit(digitdata, algorithm = \"MM\")\n",
    "  benchmark(dirmultfit(digitdata, algorithm = \"MM\"))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "EM-MM hybrid algorithm\n",
    "\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) \\geq\n",
    "\\sum_i ln \\binom{\\mathbf{m}_i}{\\mathbf{x}_i} + \\sum_i\\sum_j (x_{ij}+\\alpha_j-1) \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_j ln \\Gamma(\\alpha_j) + c^{(t)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\sum_j \\{ \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|) + \\Psi(\\alpha^{(t)}) \\right] - n ln \\Gamma(\\alpha_j) \\}+ c^{(t)}\n",
    "$$\n",
    "\n",
    "In the above new minorizing function, the parameters $\\alpha_j$ are separated and only need to be optimized independently. Equating the partial derivatives with respect to αj to 0 gives the function to solve for $\\alpha_j$\n",
    "\n",
    "$$\\sum_i \\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(\\alpha_j) = \\sum_i \\Psi(m_{i}+|\\alpha^{(t)}|) - n\\Psi(|\\alpha^{(t)}|)$$\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "$$\\Psi(\\alpha_j^{(t)}) - \\Psi(\\alpha_j) = \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=0}^{m_i-1} \\frac{1}{|\\alpha^{(t)}|+k } -  \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=0}^{x_{ij}-1} \\frac{1}{\\alpha_j^{(t)}+k }$$\n",
    "$$= \\frac{1}{n} \\sum_{k=0}^{max_i m_i-1} \\frac{r_k}{|\\alpha^{(t)}|+k } -  \\frac{1}{n} \\sum_{k=0}^{max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k }$$\n",
    "\n",
    "$$\\alpha_{(m+1)} = \\alpha_{(m)} - \\frac{\\Psi(\\alpha_{(m)})-\\alpha}{\\psi(\\alpha_{(m)})}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
